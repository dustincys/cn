---
layout: post 
category : 学术
title: 多重比较纠正和其局限性
matheq: yes
comments: :yes
share: no
tags : [多重比较, 邦费罗尼纠正, šidák纠正, 生物信息, 统计, 读书并非无用论证明] 
share: yes
---

在同一个数据集上做的假设检验越多，那么出现拒绝真假设的错误就越多。
正如对于某个特定的人，言多必失。 
有意思的是，不让此人说话如同杀了他，那怎么办？
只能让他严谨点说。
严谨到什么程度？
也就是，此人要通过什么方法来控制说错话的概率（\\(FWER\\)）小到一个特定程度\\(\\alpha\\)？
设此人说的一段话中含有\\(m\\)个断定（就是假设）所对应的\\(p\\)-value为\\(p\_1,\\ldots , p\_m\\)。
\\(I\_0\\)为这段话中正确的断定集合，大小为\\(m\_0\\)。
邦费罗尼纠正的方法是拒绝每一个\\(p\_i < \\frac{\\alpha}{m}\\)对应的断定。
因为拒绝了\\(I\_0\\)中任何一个的概率为
$$FWER=P\\left\\{ \\bigcup\\limits\_{I\_0} \\left(p\_i \\le \\frac{\\alpha}{m} \\right)\\right\\} \\le \\sum\\limits\_{I\_0} \\left\\{ p \\left(p\_i\\le \\frac{\\alpha}{m}\\right) \\right\\} \\le m\_0 \\frac{\\alpha}{m} \\le m \\frac{\\alpha}{m} = \\alpha$$
Šidák纠正的方法假设各个假设独立，这样概率就可以直接相乘。
所以只要直接拒绝每一个\\(p\_i < 1-(1-\\alpha)^{\\frac{1}{m}}\\)对应的断定，就可使\\(FWER\\)小到特定程度\\(\\alpha\\)。
以上两种纠正方法属于“不求有功，但求无过”型，过于保守。
更细点说，邦费罗尼纠正属于“比较无知型”纠正（不知道独不独立）。
Šidák纠正还算好点，读点书，有点文化修养，还知道独立这回事。
所以直观上“无知型”自然要更保守。
这个“读书并非无用论”可通过级数展开得到：
设\\(f(\\alpha)= 1-(1-\\alpha)^{\\frac{1}{m}}\\)，由于\\(0 \\le \\alpha \\le 1\\)，将该式在\\(0\\)处泰勒级数展开得到：

\\[f(\\alpha)=f(0)+f^{(1)}(0)\\alpha+\\frac{f^{(2)}}{2!}\\alpha ^ 2 + \\cdots\\]

带入公式得到

$$ f(\\alpha) = 0 + \\frac{\\alpha}{m} + \\cdots$$ 

所以\\(\\frac{\\alpha}{m} \\le 1-(1-\\alpha)^{\\frac{1}{m}} \\)，邦费罗尼纠正更保守。

两种假设中都有参数\\(n\\)，这个参数直接导致了下面情况的发生：两个人各自说一段话（各自含有的判断个数不相同）中对相同一个问题做判断时，两个人得出的结论可能不同。
在生物信息这种情况时常发生，基因组范围关系研究中，对于不同的数据集，会有得出结果不一致的情况。
比如在对单核苷酸变异与疾病关系研究中，在两个不一样大小的数据集中，相同的单核酸变异与某个疾病的关联关系可能相互矛盾。
一种方法是大家把数据集合大小固定，即，\\(n\\)设置成固定值。
另一种方法是降低多重比对出现的结果的组合的可能性，即人为地假设\\(m\_0\\)只能在\\([0, k]\\)上取值（\\(k \\le m\\)，且为整数）。
第一种方法简单粗暴一刀切，共同贫穷，并未解决保守性（未增加新知识）；
二种方法是无赖地强加了新知识，让这些数据只有“有限度地自由”，这样不同的数据集对同一个问题进行判断时候，降低了数据集合大小对判断该问题真伪的影响。
第二种方法进行多重比较具体分为两个步骤：首先计算出现的有限制的情况的概率，然后再计算某个特定问题属于某个情况的概率，选取概率最大的情况。
